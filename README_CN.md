# Clashä»£ç†å®¢æˆ·ç«¯ - çˆ¬è™«IPé˜²å°è§£å†³æ–¹æ¡ˆ

ä¸€ä¸ªä¸“ä¸ºç½‘ç»œçˆ¬è™«è®¾è®¡çš„ç»¼åˆæ€§Clashä»£ç†å®¢æˆ·ç«¯ï¼Œé€šè¿‡è‡ªåŠ¨ä»£ç†è½®æ¢å’Œè´Ÿè½½å‡è¡¡æ¥é¿å…IPè¢«å°ç¦ã€‚

## ğŸŒŸ æ ¸å¿ƒç‰¹æ€§

- **è‡ªåŠ¨èŠ‚ç‚¹è·å–**ï¼šä»getNodeé¡¹ç›®å’Œå…¶ä»–æºè‡ªåŠ¨è·å–å…è´¹ä»£ç†èŠ‚ç‚¹
- **å¤šæ ¼å¼æ”¯æŒ**ï¼šæ”¯æŒClashå’ŒV2RayèŠ‚ç‚¹æ ¼å¼ï¼Œè‡ªåŠ¨è½¬æ¢
- **å¥åº·ç›‘æ§**ï¼šå…¨é¢çš„å¥åº·æ£€æŸ¥ï¼ŒåŒ…å«å»¶è¿Ÿæµ‹é‡å’ŒæˆåŠŸç‡è·Ÿè¸ª
- **è´Ÿè½½å‡è¡¡**ï¼šå¤šç§ç­–ç•¥åŒ…æ‹¬å¥åº·æƒé‡ã€è½®è¯¢ã€æœ€å°‘ä½¿ç”¨å’Œéšæœºé€‰æ‹©
- **è‡ªåŠ¨æ•…éšœè½¬ç§»**ï¼šæ™ºèƒ½ä»£ç†åˆ‡æ¢ï¼ŒèŠ‚ç‚¹å¤±æ•ˆæ—¶è‡ªåŠ¨åˆ‡æ¢
- **ç®€æ˜“é›†æˆ**ï¼šç®€å•çš„APIï¼Œæ˜“äºä¸ç°æœ‰çˆ¬è™«æ¡†æ¶é›†æˆ
- **åå°æ›´æ–°**ï¼šè‡ªåŠ¨ä»£ç†åˆ—è¡¨æ›´æ–°å’Œå¥åº·ç›‘æ§
- **é…ç½®ç®¡ç†**ï¼šåŸºäºæ¨¡æ¿çš„é…ç½®ç”Ÿæˆï¼Œæ”¯æŒå¤‡ä»½

## ğŸ“¦ å®‰è£…é…ç½®

### å‰ç½®è¦æ±‚

1. **å®‰è£…ClashäºŒè¿›åˆ¶æ–‡ä»¶**
   ```bash
   # æ–¹å¼1ï¼šä¸‹è½½å®˜æ–¹ç‰ˆæœ¬
   # è®¿é—® https://github.com/Dreamacro/clash/releases
   
   # æ–¹å¼2ï¼šä¸‹è½½Mihomo (clash-verge-revæ ¸å¿ƒ)
   # è®¿é—® https://github.com/MetaCubeX/mihomo/releases
   
   # æ–¹å¼3ï¼šä½¿ç”¨åŒ…ç®¡ç†å™¨ (ä»¥Ubuntuä¸ºä¾‹)
   wget https://github.com/MetaCubeX/mihomo/releases/download/v1.18.0/mihomo-linux-amd64-v1.18.0.gz
   gunzip mihomo-linux-amd64-v1.18.0.gz
   chmod +x mihomo-linux-amd64-v1.18.0
   sudo mv mihomo-linux-amd64-v1.18.0 /usr/local/bin/mihomo
   ```

2. **å®‰è£…Pythonä¾èµ–**
   ```bash
   pip install aiohttp pyyaml psutil requests
   ```

### å¿«é€Ÿå¼€å§‹

```python
import asyncio
from nautilus_trader.adapters.clash import ClashProxyClient

async def main():
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    client = ClashProxyClient(config_dir='./clash_configs')
    
    try:
        # å¯åŠ¨å®¢æˆ·ç«¯ï¼ˆçˆ¬è™«ä¼˜åŒ–é…ç½®ï¼‰
        await client.start(config_type='scraping')
        
        # è·å–ä»£ç†URL
        proxy_url = await client.get_proxy_url()
        print(f"ä»£ç†åœ°å€: {proxy_url}")
        
        # ä½¿ç”¨ä»£ç†è¿›è¡Œè¯·æ±‚
        import aiohttp
        async with aiohttp.ClientSession() as session:
            async with session.get(
                'http://httpbin.org/ip',
                proxy=proxy_url
            ) as response:
                data = await response.json()
                print(f"å½“å‰IP: {data['origin']}")
    
    finally:
        await client.stop()

# è¿è¡Œç¤ºä¾‹
asyncio.run(main())
```

## ğŸš€ ä¸çˆ¬è™«æ¡†æ¶é›†æˆ

### 1. ä¸aiohttpé›†æˆï¼ˆæ¨èï¼‰

```python
import asyncio
import aiohttp
from nautilus_trader.adapters.clash import ClashProxyClient

class ProxySpider:
    def __init__(self):
        self.proxy_client = ClashProxyClient()
        self.session = None
    
    async def start(self):
        """å¯åŠ¨çˆ¬è™«å’Œä»£ç†å®¢æˆ·ç«¯"""
        # å¯åŠ¨ä»£ç†å®¢æˆ·ç«¯
        await self.proxy_client.start(config_type='scraping')
        
        # åˆ›å»ºHTTPä¼šè¯
        self.session = aiohttp.ClientSession()
        print("çˆ¬è™«å’Œä»£ç†å®¢æˆ·ç«¯å·²å¯åŠ¨")
    
    async def fetch_with_proxy(self, url, max_retries=3):
        """ä½¿ç”¨ä»£ç†è·å–URLå†…å®¹"""
        for attempt in range(max_retries):
            try:
                # è·å–ä»£ç†URLï¼ˆä½¿ç”¨å¥åº·æƒé‡ç­–ç•¥ï¼‰
                proxy_url = await self.proxy_client.get_proxy_url(
                    strategy='health_weighted'
                )
                
                if not proxy_url:
                    print(f"ç¬¬{attempt+1}æ¬¡å°è¯•ï¼šæ— å¯ç”¨ä»£ç†")
                    await asyncio.sleep(2)
                    continue
                
                # å‘èµ·è¯·æ±‚
                async with self.session.get(
                    url,
                    proxy=proxy_url,
                    timeout=aiohttp.ClientTimeout(total=15)
                ) as response:
                    if response.status == 200:
                        content = await response.text()
                        print(f"æˆåŠŸè·å– {url}ï¼Œä½¿ç”¨ä»£ç†: {proxy_url}")
                        return content
                    else:
                        print(f"HTTPé”™è¯¯ {response.status}ï¼Œåˆ‡æ¢ä»£ç†é‡è¯•")
                        
            except asyncio.TimeoutError:
                print(f"ç¬¬{attempt+1}æ¬¡å°è¯•è¶…æ—¶ï¼Œåˆ‡æ¢ä»£ç†")
                # åˆ‡æ¢åˆ°ä¸åŒçš„ä»£ç†
                await self.proxy_client.switch_proxy(strategy='round_robin')
                
            except Exception as e:
                print(f"ç¬¬{attempt+1}æ¬¡å°è¯•å¤±è´¥: {e}")
            
            await asyncio.sleep(1)
        
        print(f"è·å– {url} å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡")
        return None
    
    async def crawl_urls(self, urls):
        """æ‰¹é‡çˆ¬å–URLåˆ—è¡¨"""
        results = []
        
        for i, url in enumerate(urls):
            print(f"æ­£åœ¨çˆ¬å–ç¬¬{i+1}/{len(urls)}ä¸ªURL: {url}")
            
            content = await self.fetch_with_proxy(url)
            if content:
                results.append({
                    'url': url,
                    'content': content[:200] + '...',  # åªæ˜¾ç¤ºå‰200å­—ç¬¦
                    'status': 'success'
                })
            else:
                results.append({
                    'url': url,
                    'content': None,
                    'status': 'failed'
                })
            
            # è¯·æ±‚é—´éš”ï¼Œé¿å…è¿‡äºé¢‘ç¹
            await asyncio.sleep(2)
        
        return results
    
    async def stop(self):
        """åœæ­¢çˆ¬è™«å’Œä»£ç†å®¢æˆ·ç«¯"""
        if self.session:
            await self.session.close()
        await self.proxy_client.stop()
        print("çˆ¬è™«å’Œä»£ç†å®¢æˆ·ç«¯å·²åœæ­¢")

# ä½¿ç”¨ç¤ºä¾‹
async def spider_example():
    spider = ProxySpider()
    
    try:
        await spider.start()
        
        # è¦çˆ¬å–çš„URLåˆ—è¡¨
        urls = [
            'http://httpbin.org/ip',
            'http://httpbin.org/user-agent',
            'http://httpbin.org/headers',
            'https://api.github.com',
            'http://httpbin.org/get?test=1'
        ]
        
        # å¼€å§‹çˆ¬å–
        results = await spider.crawl_urls(urls)
        
        # æ˜¾ç¤ºç»“æœ
        print(f"\nçˆ¬å–å®Œæˆï¼ŒæˆåŠŸ: {sum(1 for r in results if r['status'] == 'success')}/{len(results)}")
        
    finally:
        await spider.stop()

# è¿è¡Œçˆ¬è™«ç¤ºä¾‹
# asyncio.run(spider_example())
```

### 2. ä¸requestsé›†æˆï¼ˆåŒæ­¥æ–¹å¼ï¼‰

```python
import requests
import asyncio
import time
from nautilus_trader.adapters.clash import ClashProxyClient

class SyncProxySpider:
    def __init__(self):
        self.proxy_client = None
        self.current_proxy = None
    
    def start(self):
        """å¯åŠ¨ä»£ç†å®¢æˆ·ç«¯"""
        async def _start():
            self.proxy_client = ClashProxyClient()
            await self.proxy_client.start(config_type='scraping')
            self.current_proxy = await self.proxy_client.get_proxy_url()
        
        asyncio.run(_start())
        print(f"ä»£ç†å®¢æˆ·ç«¯å·²å¯åŠ¨ï¼Œå½“å‰ä»£ç†: {self.current_proxy}")
    
    def get_new_proxy(self):
        """è·å–æ–°çš„ä»£ç†"""
        async def _get_proxy():
            return await self.proxy_client.get_proxy_url(strategy='round_robin')
        
        self.current_proxy = asyncio.run(_get_proxy())
        return self.current_proxy
    
    def fetch_with_proxy(self, url, max_retries=3):
        """ä½¿ç”¨ä»£ç†è·å–URL"""
        for attempt in range(max_retries):
            try:
                if not self.current_proxy:
                    self.get_new_proxy()
                
                proxies = {
                    'http': self.current_proxy,
                    'https': self.current_proxy
                }
                
                response = requests.get(
                    url,
                    proxies=proxies,
                    timeout=15,
                    headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
                )
                
                if response.status_code == 200:
                    print(f"æˆåŠŸè·å– {url}")
                    return response.text
                else:
                    print(f"HTTPé”™è¯¯ {response.status_code}ï¼Œåˆ‡æ¢ä»£ç†é‡è¯•")
                    self.get_new_proxy()
                    
            except requests.exceptions.Timeout:
                print(f"ç¬¬{attempt+1}æ¬¡å°è¯•è¶…æ—¶ï¼Œåˆ‡æ¢ä»£ç†")
                self.get_new_proxy()
                
            except Exception as e:
                print(f"ç¬¬{attempt+1}æ¬¡å°è¯•å¤±è´¥: {e}")
                self.get_new_proxy()
            
            time.sleep(1)
        
        return None
    
    def stop(self):
        """åœæ­¢ä»£ç†å®¢æˆ·ç«¯"""
        async def _stop():
            if self.proxy_client:
                await self.proxy_client.stop()
        
        asyncio.run(_stop())
        print("ä»£ç†å®¢æˆ·ç«¯å·²åœæ­¢")

# ä½¿ç”¨ç¤ºä¾‹
def sync_spider_example():
    spider = SyncProxySpider()
    
    try:
        spider.start()
        
        urls = [
            'http://httpbin.org/ip',
            'http://httpbin.org/user-agent',
            'https://api.github.com'
        ]
        
        for url in urls:
            content = spider.fetch_with_proxy(url)
            if content:
                print(f"è·å–åˆ°å†…å®¹é•¿åº¦: {len(content)}")
            time.sleep(2)
    
    finally:
        spider.stop()
```

### 3. ä¸Scrapyé›†æˆ

```python
# middlewares.py
import asyncio
from scrapy.downloadermiddlewares.httpproxy import HttpProxyMiddleware
from scrapy.exceptions import NotConfigured
from nautilus_trader.adapters.clash import ClashProxyClient

class ClashProxyMiddleware:
    def __init__(self):
        self.proxy_client = None
        self.loop = None
    
    @classmethod
    def from_crawler(cls, crawler):
        return cls()
    
    def open_spider(self, spider):
        """çˆ¬è™«å¯åŠ¨æ—¶åˆå§‹åŒ–ä»£ç†å®¢æˆ·ç«¯"""
        async def _start_proxy():
            self.proxy_client = ClashProxyClient()
            await self.proxy_client.start(config_type='scraping')
        
        # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
        self.loop = asyncio.new_event_loop()
        asyncio.set_event_loop(self.loop)
        self.loop.run_until_complete(_start_proxy())
        
        spider.logger.info("Clashä»£ç†ä¸­é—´ä»¶å·²å¯åŠ¨")
    
    def close_spider(self, spider):
        """çˆ¬è™«å…³é—­æ—¶åœæ­¢ä»£ç†å®¢æˆ·ç«¯"""
        async def _stop_proxy():
            if self.proxy_client:
                await self.proxy_client.stop()
        
        if self.loop:
            self.loop.run_until_complete(_stop_proxy())
            self.loop.close()
        
        spider.logger.info("Clashä»£ç†ä¸­é—´ä»¶å·²åœæ­¢")
    
    def process_request(self, request, spider):
        """ä¸ºæ¯ä¸ªè¯·æ±‚è®¾ç½®ä»£ç†"""
        async def _get_proxy():
            return await self.proxy_client.get_proxy_url(strategy='health_weighted')
        
        try:
            proxy_url = self.loop.run_until_complete(_get_proxy())
            if proxy_url:
                request.meta['proxy'] = proxy_url
                spider.logger.debug(f"ä½¿ç”¨ä»£ç†: {proxy_url}")
        except Exception as e:
            spider.logger.error(f"è·å–ä»£ç†å¤±è´¥: {e}")
        
        return None

# settings.py é…ç½®
DOWNLOADER_MIDDLEWARES = {
    'myproject.middlewares.ClashProxyMiddleware': 350,
}

# çˆ¬è™«ç¤ºä¾‹
import scrapy

class ExampleSpider(scrapy.Spider):
    name = 'example'
    start_urls = [
        'http://httpbin.org/ip',
        'http://httpbin.org/user-agent',
        'http://httpbin.org/headers'
    ]
    
    def parse(self, response):
        self.logger.info(f"å“åº”çŠ¶æ€: {response.status}")
        self.logger.info(f"å“åº”å†…å®¹: {response.text[:200]}")
        
        yield {
            'url': response.url,
            'status': response.status,
            'content_length': len(response.text)
        }
```

## ğŸ”§ é«˜çº§é…ç½®

### è´Ÿè½½å‡è¡¡ç­–ç•¥

```python
# å¥åº·æƒé‡ç­–ç•¥ï¼ˆæ¨èç”¨äºçˆ¬è™«ï¼‰
proxy_url = await client.get_proxy_url(strategy='health_weighted')

# è½®è¯¢ç­–ç•¥ï¼ˆå‡åŒ€åˆ†å¸ƒè¯·æ±‚ï¼‰
proxy_url = await client.get_proxy_url(strategy='round_robin')

# æœ€å°‘ä½¿ç”¨ç­–ç•¥ï¼ˆè´Ÿè½½å‡è¡¡ï¼‰
proxy_url = await client.get_proxy_url(strategy='least_used')

# éšæœºç­–ç•¥ï¼ˆç®€å•éšæœºé€‰æ‹©ï¼‰
proxy_url = await client.get_proxy_url(strategy='random')
```

### é…ç½®ç±»å‹é€‰æ‹©

```python
# çˆ¬è™«ä¼˜åŒ–é…ç½®ï¼ˆé»˜è®¤ï¼ŒåŒ…å«æ•…éšœè½¬ç§»å’Œè´Ÿè½½å‡è¡¡ï¼‰
await client.start(config_type='scraping')

# é€Ÿåº¦ä¼˜åŒ–é…ç½®ï¼ˆé€‰æ‹©æœ€å¿«çš„ä»£ç†ï¼‰
await client.start(config_type='speed')

# é€šç”¨é…ç½®ï¼ˆå¹³è¡¡æ€§èƒ½å’Œç¨³å®šæ€§ï¼‰
await client.start(config_type='general')
```

### ç›‘æ§å’Œç»Ÿè®¡

```python
# è·å–ä»£ç†ç»Ÿè®¡ä¿¡æ¯
info = await client.get_proxy_info()
print(f"æ€»ä»£ç†æ•°: {info['proxy_stats']['total_proxies']}")
print(f"å¥åº·ä»£ç†æ•°: {info['proxy_stats']['healthy_proxies']}")
print(f"å¥åº·ç‡: {info['proxy_stats']['health_rate']:.1%}")

# æµ‹è¯•ç‰¹å®šä»£ç†
test_result = await client.test_proxy('proxy_name')
if test_result['success']:
    print(f"ä»£ç†IP: {test_result['ip']}")
    print(f"å»¶è¿Ÿ: {test_result['latency']}ms")
```

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **æ‰¾ä¸åˆ°ClashäºŒè¿›åˆ¶æ–‡ä»¶**
   ```bash
   # è§£å†³æ–¹æ¡ˆ1ï¼šå®‰è£…åˆ°ç³»ç»ŸPATH
   sudo cp mihomo /usr/local/bin/
   
   # è§£å†³æ–¹æ¡ˆ2ï¼šæŒ‡å®šè·¯å¾„
   client = ClashProxyClient(clash_binary_path='/path/to/clash')
   ```

2. **æ²¡æœ‰å¥åº·çš„ä»£ç†**
   ```python
   # æ£€æŸ¥ä»£ç†çŠ¶æ€
   info = await client.get_proxy_info()
   if info['proxy_stats']['healthy_proxies'] == 0:
       print("ç­‰å¾…ä»£ç†å¥åº·æ£€æŸ¥å®Œæˆ...")
       await asyncio.sleep(30)
   ```

3. **è¯·æ±‚è¶…æ—¶é¢‘ç¹**
   ```python
   # è°ƒæ•´è¶…æ—¶è®¾ç½®
   async with session.get(url, proxy=proxy_url, timeout=aiohttp.ClientTimeout(total=30)) as response:
       pass
   ```

### è°ƒè¯•æ¨¡å¼

```python
import logging

# å¯ç”¨è¯¦ç»†æ—¥å¿—
logging.basicConfig(level=logging.DEBUG)

# æˆ–è€…åªå¯ç”¨Clashç›¸å…³æ—¥å¿—
logging.getLogger('nautilus_trader.adapters.clash').setLevel(logging.DEBUG)
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **åˆç†è®¾ç½®è¯·æ±‚é—´éš”**ï¼šé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚å¯¼è‡´IPè¢«å°
2. **ä½¿ç”¨å¥åº·æƒé‡ç­–ç•¥**ï¼šä¼˜å…ˆä½¿ç”¨æ€§èƒ½å¥½çš„ä»£ç†
3. **å¯ç”¨è‡ªåŠ¨æ›´æ–°**ï¼šä¿æŒä»£ç†åˆ—è¡¨çš„æ–°é²œåº¦
4. **ç›‘æ§å¥åº·çŠ¶æ€**ï¼šå®šæœŸæ£€æŸ¥ä»£ç†å¥åº·ç‡
5. **è®¾ç½®åˆç†è¶…æ—¶**ï¼šå¹³è¡¡é€Ÿåº¦å’ŒæˆåŠŸç‡

## ğŸ¯ æœ€ä½³å®è·µ

1. **é”™è¯¯å¤„ç†**ï¼šå§‹ç»ˆåŒ…å«é‡è¯•é€»è¾‘å’Œå¼‚å¸¸å¤„ç†
2. **ä»£ç†è½®æ¢**ï¼šå®šæœŸåˆ‡æ¢ä»£ç†é¿å…å•ä¸€IPè¿‡åº¦ä½¿ç”¨
3. **è¯·æ±‚å¤´è®¾ç½®**ï¼šä½¿ç”¨çœŸå®çš„User-Agentå’Œå…¶ä»–è¯·æ±‚å¤´
4. **é€Ÿç‡é™åˆ¶**ï¼šæ§åˆ¶è¯·æ±‚é¢‘ç‡ï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º
5. **ç›‘æ§æ—¥å¿—**ï¼šå…³æ³¨ä»£ç†å¥åº·çŠ¶æ€å’Œé”™è¯¯ä¿¡æ¯

## ğŸ¯ å®é™…åº”ç”¨åœºæ™¯

### åœºæ™¯1ï¼šç”µå•†æ•°æ®é‡‡é›†

```python
from nautilus_trader.adapters.clash.simple_spider import SimpleProxySpider
import time

def crawl_product_prices():
    """çˆ¬å–ç”µå•†äº§å“ä»·æ ¼"""
    spider = SimpleProxySpider()

    try:
        spider.start()

        # äº§å“URLåˆ—è¡¨
        product_urls = [
            'https://example-shop.com/product/123',
            'https://example-shop.com/product/456',
            # ... æ›´å¤šäº§å“URL
        ]

        for url in product_urls:
            response = spider.get(url)
            if response:
                # è§£æä»·æ ¼ä¿¡æ¯
                # price = parse_price(response.text)
                print(f"æˆåŠŸè·å–äº§å“é¡µé¢: {url}")

            time.sleep(3)  # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹

    finally:
        spider.stop()
```

### åœºæ™¯2ï¼šæ–°é—»èµ„è®¯ç›‘æ§

```python
from nautilus_trader.adapters.clash.spider_integration import AdvancedProxySpider
import asyncio

async def monitor_news():
    """ç›‘æ§æ–°é—»ç½‘ç«™æ›´æ–°"""
    spider = AdvancedProxySpider()

    try:
        await spider.start()

        news_sites = [
            'https://news-site1.com/latest',
            'https://news-site2.com/breaking',
            # ... æ›´å¤šæ–°é—»æº
        ]

        articles = await spider.crawl_news_articles(news_sites)

        # ä¿å­˜åˆ°æ•°æ®åº“æˆ–æ–‡ä»¶
        await spider.save_results(articles, 'latest_news.json')

    finally:
        await spider.stop()

# å®šæ—¶è¿è¡Œ
# asyncio.run(monitor_news())
```

### åœºæ™¯3ï¼šAPIæ•°æ®é‡‡é›†

```python
def collect_api_data():
    """é‡‡é›†APIæ•°æ®"""
    spider = SimpleProxySpider()

    try:
        spider.start()

        # APIæ¥å£åˆ—è¡¨
        api_endpoints = [
            'https://api.example.com/v1/data',
            'https://api.example.com/v1/stats',
            # ... æ›´å¤šAPI
        ]

        for endpoint in api_endpoints:
            # æ·»åŠ APIå¯†é’¥ç­‰è®¤è¯ä¿¡æ¯
            headers = {
                'Authorization': 'Bearer YOUR_API_KEY',
                'Accept': 'application/json'
            }

            json_data = spider.get_json(endpoint, headers=headers)
            if json_data:
                print(f"æˆåŠŸè·å–APIæ•°æ®: {endpoint}")
                # å¤„ç†æ•°æ®
                # process_api_data(json_data)

    finally:
        spider.stop()
```

### åœºæ™¯4ï¼šç¤¾äº¤åª’ä½“æ•°æ®é‡‡é›†

```python
async def crawl_social_media():
    """çˆ¬å–ç¤¾äº¤åª’ä½“æ•°æ®"""
    spider = AdvancedProxySpider()

    try:
        await spider.start()

        # ç¤¾äº¤åª’ä½“é¡µé¢
        social_urls = [
            'https://social-platform.com/user/profile1',
            'https://social-platform.com/user/profile2',
            # ... æ›´å¤šç”¨æˆ·é¡µé¢
        ]

        for url in social_urls:
            result = await spider.fetch_with_retry(url, max_retries=5)
            if result:
                # è§£æç”¨æˆ·ä¿¡æ¯ã€å¸–å­ç­‰
                # user_data = parse_social_profile(result['content'])
                print(f"æˆåŠŸè·å–ç¤¾äº¤åª’ä½“æ•°æ®: {url}")

            # è¾ƒé•¿çš„é—´éš”ï¼Œé¿å…è¢«æ£€æµ‹
            await asyncio.sleep(random.uniform(5, 10))

    finally:
        await spider.stop()
```

## ğŸ”„ ä¸ç°æœ‰é¡¹ç›®é›†æˆ

### é›†æˆåˆ°Djangoé¡¹ç›®

```python
# views.py
from django.http import JsonResponse
from nautilus_trader.adapters.clash.simple_spider import SimpleProxySpider
import threading
import time

class DataCollectionView:
    def __init__(self):
        self.spider = SimpleProxySpider()
        self.spider.start()

    def collect_data(self, request):
        """æ•°æ®é‡‡é›†æ¥å£"""
        urls = request.POST.getlist('urls')

        results = []
        for url in urls:
            response = self.spider.get(url)
            if response:
                results.append({
                    'url': url,
                    'status': 'success',
                    'data': response.text[:500]  # æˆªå–å‰500å­—ç¬¦
                })
            else:
                results.append({
                    'url': url,
                    'status': 'failed'
                })

        return JsonResponse({'results': results})

    def __del__(self):
        if hasattr(self, 'spider'):
            self.spider.stop()
```

### é›†æˆåˆ°Flaské¡¹ç›®

```python
# app.py
from flask import Flask, request, jsonify
from nautilus_trader.adapters.clash.simple_spider import SimpleProxySpider

app = Flask(__name__)

# å…¨å±€çˆ¬è™«å®ä¾‹
spider = SimpleProxySpider()

@app.before_first_request
def initialize():
    """åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–çˆ¬è™«"""
    spider.start()

@app.route('/crawl', methods=['POST'])
def crawl_data():
    """çˆ¬å–æ•°æ®æ¥å£"""
    data = request.get_json()
    urls = data.get('urls', [])

    results = spider.crawl_urls(urls, delay=1.0)

    return jsonify({
        'total': len(results),
        'successful': sum(1 for r in results if r['success']),
        'results': results
    })

@app.route('/proxy-status')
def proxy_status():
    """ä»£ç†çŠ¶æ€æ¥å£"""
    info = spider.get_proxy_info()
    return jsonify(info)

@app.teardown_appcontext
def cleanup(error):
    """åº”ç”¨å…³é—­æ—¶æ¸…ç†èµ„æº"""
    spider.stop()

if __name__ == '__main__':
    app.run(debug=True)
```

### é›†æˆåˆ°Celeryä»»åŠ¡

```python
# tasks.py
from celery import Celery
from nautilus_trader.adapters.clash.simple_spider import SimpleProxySpider

app = Celery('crawler')

@app.task
def crawl_urls_task(urls):
    """å¼‚æ­¥çˆ¬å–ä»»åŠ¡"""
    spider = SimpleProxySpider()

    try:
        spider.start()
        results = spider.crawl_urls(urls, delay=2.0)

        return {
            'status': 'completed',
            'total': len(results),
            'successful': sum(1 for r in results if r['success']),
            'results': results
        }

    finally:
        spider.stop()

# ä½¿ç”¨æ–¹å¼
# from tasks import crawl_urls_task
# result = crawl_urls_task.delay(['http://example.com', 'http://example2.com'])
```

## ğŸ“Š ç›‘æ§å’Œç»´æŠ¤

### å¥åº·ç›‘æ§è„šæœ¬

```python
# monitor.py
import asyncio
import time
from nautilus_trader.adapters.clash import ClashProxyClient

async def monitor_proxy_health():
    """ç›‘æ§ä»£ç†å¥åº·çŠ¶æ€"""
    client = ClashProxyClient()

    try:
        await client.start()

        while True:
            info = await client.get_proxy_info()
            stats = info['proxy_stats']
            health_stats = info['health_stats']

            print(f"æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"æ€»ä»£ç†: {stats['total_proxies']}")
            print(f"å¥åº·ä»£ç†: {stats['healthy_proxies']}")
            print(f"å¥åº·ç‡: {stats['health_rate']:.1%}")
            print(f"å¹³å‡åˆ†æ•°: {health_stats['average_score']}")
            print("-" * 40)

            # å¦‚æœå¥åº·ç‡è¿‡ä½ï¼Œå‘é€å‘Šè­¦
            if stats['health_rate'] < 0.3:
                print("âš ï¸ è­¦å‘Šï¼šä»£ç†å¥åº·ç‡è¿‡ä½ï¼")
                # è¿™é‡Œå¯ä»¥æ·»åŠ é‚®ä»¶æˆ–çŸ­ä¿¡å‘Šè­¦

            await asyncio.sleep(300)  # æ¯5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡

    finally:
        await client.stop()

if __name__ == "__main__":
    asyncio.run(monitor_proxy_health())
```

### è‡ªåŠ¨é‡å¯è„šæœ¬

```python
# auto_restart.py
import asyncio
import logging
import time
from nautilus_trader.adapters.clash import ClashProxyClient

async def auto_restart_on_failure():
    """ä»£ç†å®¢æˆ·ç«¯è‡ªåŠ¨é‡å¯"""
    while True:
        client = ClashProxyClient()

        try:
            success = await client.start()
            if not success:
                logging.error("ä»£ç†å®¢æˆ·ç«¯å¯åŠ¨å¤±è´¥ï¼Œç­‰å¾…é‡è¯•...")
                await asyncio.sleep(60)
                continue

            logging.info("ä»£ç†å®¢æˆ·ç«¯å¯åŠ¨æˆåŠŸ")

            # ç›‘æ§è¿è¡ŒçŠ¶æ€
            while True:
                try:
                    info = await client.get_proxy_info()
                    if not info['is_running']:
                        logging.warning("ä»£ç†å®¢æˆ·ç«¯å·²åœæ­¢è¿è¡Œ")
                        break

                    await asyncio.sleep(30)  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡

                except Exception as e:
                    logging.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
                    break

        except Exception as e:
            logging.error(f"ä»£ç†å®¢æˆ·ç«¯è¿è¡Œå¼‚å¸¸: {e}")

        finally:
            try:
                await client.stop()
            except:
                pass

        logging.info("ç­‰å¾…é‡å¯...")
        await asyncio.sleep(30)

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(auto_restart_on_failure())
```

## ğŸ¯ æ€§èƒ½è°ƒä¼˜å»ºè®®

1. **ä»£ç†æ± å¤§å°**ï¼šä¿æŒ50-100ä¸ªæ´»è·ƒä»£ç†ä¸ºæœ€ä½³
2. **è¯·æ±‚é¢‘ç‡**ï¼šå»ºè®®æ¯ä¸ªè¯·æ±‚é—´éš”2-5ç§’
3. **é‡è¯•ç­–ç•¥**ï¼šæœ€å¤§é‡è¯•3æ¬¡ï¼ŒæŒ‡æ•°é€€é¿
4. **å¥åº·æ£€æŸ¥**ï¼šæ¯5åˆ†é’Ÿè¿›è¡Œä¸€æ¬¡å…¨é¢å¥åº·æ£€æŸ¥
5. **ä»£ç†è½®æ¢**ï¼šæ¯10-20ä¸ªè¯·æ±‚åˆ‡æ¢ä¸€æ¬¡ä»£ç†
6. **é”™è¯¯å¤„ç†**ï¼šå¯¹ä¸åŒHTTPçŠ¶æ€ç é‡‡ç”¨ä¸åŒçš„å¤„ç†ç­–ç•¥

é€šè¿‡ä»¥ä¸Šé…ç½®å’Œé›†æˆæ–¹å¼ï¼Œæ‚¨å¯ä»¥è½»æ¾åœ°å°†Clashä»£ç†å®¢æˆ·ç«¯ä¸ç°æœ‰çš„çˆ¬è™«é¡¹ç›®é›†æˆï¼Œæœ‰æ•ˆé¿å…IPè¢«å°ç¦çš„é—®é¢˜ã€‚
