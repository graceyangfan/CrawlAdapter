"""
æµ‹è¯•ç®€åŒ–åçš„CrawlAdapteråŠŸèƒ½

éªŒè¯åŸºäºåŸæœ‰core.pyå’Œfetchers.pyçš„ç®€åŒ–å®ç°
"""

import asyncio
import logging
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from crawladapter import ProxyClient, NodeFetcher


async def test_simplified_workflow():
    """æµ‹è¯•ç®€åŒ–çš„å·¥ä½œæµç¨‹"""
    print("ğŸ§ª æµ‹è¯•ç®€åŒ–çš„CrawlAdapterå·¥ä½œæµç¨‹")
    print("=" * 60)
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    # è‡ªå®šä¹‰èŠ‚ç‚¹æº
    custom_sources = {
        'clash': [
            'https://raw.githubusercontent.com/peasoft/NoMoreWalls/master/list.yml',
        ],
        'v2ray': []
    }
    
    # åˆ›å»ºä»£ç†å®¢æˆ·ç«¯ï¼ˆä¸æŒ‡å®šäºŒè¿›åˆ¶è·¯å¾„ï¼Œè®©ç³»ç»Ÿè‡ªåŠ¨æŸ¥æ‰¾ï¼‰
    client = ProxyClient(
        config_dir='./clash_configs',
        enable_adaptive_health_check=False,  # ä½¿ç”¨ç®€åŒ–çš„å¥åº·æ£€æŸ¥
        enable_metrics=False  # å…³é—­ç›‘æ§
    )
    
    try:
        print("\nğŸ“‹ æ­¥éª¤1: è®¾ç½®è‡ªå®šä¹‰èŠ‚ç‚¹æº")
        # è®¾ç½®è‡ªå®šä¹‰NodeFetcher
        client.node_fetcher = NodeFetcher(custom_sources=custom_sources)
        print("âœ… è‡ªå®šä¹‰èŠ‚ç‚¹æºè®¾ç½®å®Œæˆ")
        
        print("\nğŸ“‹ æ­¥éª¤2: è®¾ç½®å¥åº·æ£€æŸ¥URL")
        # è®¾ç½®å¥åº·æ£€æŸ¥URL
        health_urls = [
            'http://httpbin.org/ip',
            'http://www.gstatic.com/generate_204'
        ]
        client.set_health_check_urls(health_urls)
        print(f"âœ… è®¾ç½®äº† {len(health_urls)} ä¸ªå¥åº·æ£€æŸ¥URL")
        
        print("\nğŸ“‹ æ­¥éª¤3: å¯åŠ¨ä»£ç†å®¢æˆ·ç«¯")
        # å¯åŠ¨ä»£ç†å®¢æˆ·ç«¯ï¼ˆåŒ…å«å®Œæ•´æµç¨‹ï¼‰
        proxy_rules = [
            'panewslab.com',
            'www.panewslab.com',
            'httpbin.org'
        ]
        
        success = await client.start(
            rules=proxy_rules,
            enable_auto_update=False  # å…³é—­è‡ªåŠ¨æ›´æ–°
        )
        
        if success:
            print("âœ… ä»£ç†å®¢æˆ·ç«¯å¯åŠ¨æˆåŠŸ")
            print(f"   ä»£ç†ç«¯å£: {client.proxy_port}")
            print(f"   APIç«¯å£: {client.api_port}")
            print(f"   æ´»è·ƒä»£ç†æ•°: {len(client.active_proxies)}")
        else:
            print("âŒ ä»£ç†å®¢æˆ·ç«¯å¯åŠ¨å¤±è´¥")
            print("\nğŸ’¡ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
            print("1. è¿è¡ŒäºŒè¿›åˆ¶æ–‡ä»¶è®¾ç½®è„šæœ¬:")
            print("   python setup_clash_binary.py")
            print("2. æ‰‹åŠ¨ä¸‹è½½Mihomo:")
            print("   https://github.com/MetaCubeX/mihomo/releases")
            print("3. æˆ–è€…å®‰è£…åˆ°ç³»ç»ŸPATH:")
            print("   sudo apt install clash  # Ubuntu/Debian")
            print("   brew install clash      # macOS")
            return False
        
        print("\nğŸ“‹ æ­¥éª¤4: æµ‹è¯•ä»£ç†åŠŸèƒ½")
        # è·å–ä»£ç†URL
        proxy_url = await client.get_proxy_url()
        if proxy_url:
            print(f"âœ… è·å–ä»£ç†URL: {proxy_url}")
        else:
            print("âš ï¸ æœªèƒ½è·å–ä»£ç†URL")
        
        # è·å–ä»£ç†ä¿¡æ¯
        proxy_info = await client.get_proxy_info()
        print(f"âœ… ä»£ç†ä¿¡æ¯: {proxy_info.get('proxy_stats', {})}")
        
        print("\nğŸ“‹ æ­¥éª¤5: æµ‹è¯•ä»£ç†åˆ‡æ¢")
        # æµ‹è¯•ä»£ç†åˆ‡æ¢
        switch_success = await client.switch_proxy('round_robin')
        if switch_success:
            print("âœ… ä»£ç†åˆ‡æ¢æˆåŠŸ")
        else:
            print("âš ï¸ ä»£ç†åˆ‡æ¢å¤±è´¥æˆ–æ— å˜åŒ–")
        
        print("\nğŸ“‹ æ­¥éª¤6: æµ‹è¯•IPæ£€æŸ¥")
        # æµ‹è¯•IPæ£€æŸ¥
        try:
            import aiohttp
            async with aiohttp.ClientSession() as session:
                async with session.get("http://httpbin.org/ip", proxy=proxy_url) as response:
                    if response.status == 200:
                        data = await response.json()
                        current_ip = data.get('origin', 'unknown')
                        print(f"âœ… å½“å‰IP: {current_ip}")
                    else:
                        print(f"âš ï¸ IPæ£€æŸ¥å¤±è´¥: HTTP {response.status}")
        except Exception as e:
            print(f"âš ï¸ IPæ£€æŸ¥å¼‚å¸¸: {e}")
        
        print("\nğŸ“‹ æ­¥éª¤7: æµ‹è¯•æ–°é—»API")
        # æµ‹è¯•æ–°é—»API
        try:
            api_url = "https://www.panewslab.com/webapi/flashnews?LId=1&Rn=3&tw=0"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(api_url, proxy=proxy_url, headers=headers, timeout=aiohttp.ClientTimeout(total=20)) as response:
                    if response.status == 200:
                        data = await response.json()
                        news_count = 0
                        if isinstance(data, dict) and 'data' in data:
                            flash_news = data['data'].get('flashNews', [])
                            if flash_news and isinstance(flash_news, list):
                                news_list = flash_news[0].get('list', [])
                                news_count = len(news_list)
                        print(f"âœ… æ–°é—»APIæµ‹è¯•æˆåŠŸï¼Œè·å– {news_count} æ¡æ–°é—»")
                    else:
                        print(f"âš ï¸ æ–°é—»APIæµ‹è¯•å¤±è´¥: HTTP {response.status}")
        except Exception as e:
            print(f"âš ï¸ æ–°é—»APIæµ‹è¯•å¼‚å¸¸: {e}")
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•æ­¥éª¤å®Œæˆï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
        return False
    
    finally:
        print("\nğŸ“‹ æ­¥éª¤8: æ¸…ç†èµ„æº")
        # åœæ­¢ä»£ç†å®¢æˆ·ç«¯
        await client.stop()
        print("âœ… ä»£ç†å®¢æˆ·ç«¯å·²åœæ­¢")


async def test_basic_functionality():
    """æµ‹è¯•åŸºæœ¬åŠŸèƒ½"""
    print("\nğŸ”§ æµ‹è¯•åŸºæœ¬åŠŸèƒ½")
    print("-" * 40)
    
    # æµ‹è¯•NodeFetcher
    print("ğŸ“¡ æµ‹è¯•NodeFetcher...")
    fetcher = NodeFetcher()
    
    try:
        # è·å–å°‘é‡èŠ‚ç‚¹è¿›è¡Œæµ‹è¯•
        nodes = await fetcher.fetch_nodes('clash')
        print(f"âœ… NodeFetcheræµ‹è¯•æˆåŠŸï¼Œè·å– {len(nodes)} ä¸ªèŠ‚ç‚¹")
    except Exception as e:
        print(f"âš ï¸ NodeFetcheræµ‹è¯•å¤±è´¥: {e}")
    
    # æµ‹è¯•ProxyClientåˆ›å»º
    print("ğŸ”§ æµ‹è¯•ProxyClientåˆ›å»º...")
    try:
        client = ProxyClient()
        print("âœ… ProxyClientåˆ›å»ºæˆåŠŸ")
        print(f"   é…ç½®ç›®å½•: {client.config_dir}")
        print(f"   ä»£ç†ç«¯å£: {client.proxy_port}")
        print(f"   APIç«¯å£: {client.api_port}")
    except Exception as e:
        print(f"âŒ ProxyClientåˆ›å»ºå¤±è´¥: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ CrawlAdapter ç®€åŒ–ç‰ˆæœ¬æµ‹è¯•")
    print("åŸºäºåŸæœ‰ core.py å’Œ fetchers.py çš„ä¼˜åŒ–å®ç°")
    print("=" * 80)
    
    try:
        # åŸºæœ¬åŠŸèƒ½æµ‹è¯•
        await test_basic_functionality()
        
        # å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•
        success = await test_simplified_workflow()
        
        if success:
            print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
            print("\nğŸ“ æµ‹è¯•æ€»ç»“:")
            print("  - è‡ªåŠ¨è¿›ç¨‹ç®¡ç†: âœ…")
            print("  - èŠ‚ç‚¹è·å–: âœ…")
            print("  - å¥åº·æ£€æŸ¥: âœ…")
            print("  - é…ç½®ç”Ÿæˆ: âœ…")
            print("  - Clashå¯åŠ¨: âœ…")
            print("  - ä»£ç†åˆ‡æ¢: âœ…")
            print("  - IPç›‘æ§: âœ…")
            print("  - æ–°é—»çˆ¬å–: âœ…")
            print("  - èµ„æºæ¸…ç†: âœ…")
        else:
            print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—äº†è§£è¯¦æƒ…")
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {e}")


if __name__ == "__main__":
    asyncio.run(main())
