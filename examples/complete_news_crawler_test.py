"""
æ”¹è¿›çš„å®Œæ•´æ–°é—»çˆ¬è™«æµ‹è¯•
æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
1. ä½¿ç”¨ç°æœ‰é…ç½®æ–‡ä»¶ (config_path)
2. é€šè¿‡custom_sourcesè‡ªåŠ¨è·å–èŠ‚ç‚¹

å®ç°å®Œæ•´æµç¨‹ï¼š
1. èŠ‚ç‚¹è·å– (æ”¯æŒcustom_sourceså‚æ•°)
2. å¥åº·æ£€æŸ¥å¹¶å†™å…¥ruleä¿å­˜åˆ°config
3. å¯åŠ¨clashå¼€å§‹å¥åº·æ£€æŸ¥
4. å°†ä¸å¥åº·èŠ‚ç‚¹æ’é™¤ï¼ˆä¿å­˜å¥åº·èŠ‚ç‚¹åœ¨å†…å­˜æˆ–é‡æ–°å†™å…¥config.yamlï¼‰
5. é‡æ–°å¯åŠ¨clash
6. å¼€å§‹è‡ªåŠ¨èŠ‚ç‚¹åˆ‡æ¢æˆ–å¼ºåˆ¶åˆ‡æ¢
7. çˆ¬è™«æµ‹è¯•éªŒè¯
8. éªŒè¯å…¶ä»–ç½‘å€ä¸èµ°ä»£ç†
"""

import asyncio
import aiohttp
import subprocess
import platform
import logging
import sys
import yaml
import time
from pathlib import Path
from typing import List, Dict, Optional
from urllib.parse import urlparse

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from crawladapter.fetchers import NodeFetcher


class ImprovedCompleteNewsTest:
    """æ”¹è¿›çš„å®Œæ•´æ–°é—»çˆ¬è™«æµ‹è¯•"""

    def __init__(self,
                 config_path: Optional[str] = None,
                 custom_sources: Optional[Dict[str, List[str]]] = None,
                 min_healthy_nodes: int = 3):
        """
        åˆå§‹åŒ–æµ‹è¯•

        Args:
            config_path: ç°æœ‰é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            custom_sources: è‡ªå®šä¹‰èŠ‚ç‚¹æºï¼ˆå¯é€‰ï¼‰
            min_healthy_nodes: æœ€å°‘å¥åº·èŠ‚ç‚¹æ•°é‡
        """
        self.logger = logging.getLogger(__name__)
        self.setup_logging()

        # è·¯å¾„é…ç½®
        self.project_root = Path(__file__).parent.parent
        self.config_dir = self.project_root / 'clash_configs'
        self.config_dir.mkdir(exist_ok=True)

        # é…ç½®æ–‡ä»¶è·¯å¾„
        if config_path:
            self.config_path = Path(config_path)
        else:
            self.config_path = self.config_dir / 'auto_generated_config.yaml'

        self.binary_path = self.project_root / 'mihomo_proxy' / 'mihomo'

        # è‡ªå®šä¹‰æºé…ç½®
        self.custom_sources = custom_sources or {
            'clash': [
                'https://raw.githubusercontent.com/peasoft/NoMoreWalls/master/list.yml',
                'https://raw.githubusercontent.com/Alvin9999/pac2/master/clash/config.yaml',
            ],
            'v2ray': []
        }

        # ç½‘ç»œé…ç½®
        self.proxy_port = 7890
        self.api_port = 9090
        self.clash_api_base = f"http://127.0.0.1:{self.api_port}"

        # æµ‹è¯•é…ç½®
        self.min_healthy_nodes = min_healthy_nodes

        # ä»£ç†ç›¸å…³URLï¼ˆéœ€è¦èµ°ä»£ç†ï¼‰
        self.proxy_urls = [
            'httpbin.org',
            'www.gstatic.com',
            'detectportal.firefox.com',
            'www.msftconnecttest.com',
            'panewslab.com',
            'www.panewslab.com'
        ]

        # ç›´è¿URLï¼ˆä¸èµ°ä»£ç†ï¼‰
        self.direct_urls = [
            'baidu.com',
            'qq.com',
            'taobao.com',
            'jd.com'
        ]

        # çŠ¶æ€å˜é‡
        self.clash_process: Optional[subprocess.Popen] = None
        self.all_nodes: List[Dict] = []
        self.healthy_nodes: List[Dict] = []

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('improved_complete_news_test.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )

    async def step1_fetch_nodes(self) -> bool:
        """æ­¥éª¤1: èŠ‚ç‚¹è·å–ï¼ˆæ”¯æŒä¸¤ç§æ¨¡å¼ï¼‰"""
        self.logger.info("ğŸš€ æ­¥éª¤1: èŠ‚ç‚¹è·å–")

        try:
            # æ¨¡å¼1: å¦‚æœæœ‰ç°æœ‰é…ç½®æ–‡ä»¶ï¼Œå°è¯•è¯»å–
            if self.config_path.exists() and self.config_path.name != 'auto_generated_config.yaml':
                self.logger.info(f"ğŸ“ æ¨¡å¼1: ä»ç°æœ‰é…ç½®æ–‡ä»¶è¯»å–èŠ‚ç‚¹: {self.config_path}")

                with open(self.config_path, 'r', encoding='utf-8') as f:
                    existing_config = yaml.safe_load(f)

                self.all_nodes = existing_config.get('proxies', [])

                if self.all_nodes:
                    self.logger.info(f"âœ… ä»é…ç½®æ–‡ä»¶è¯»å– {len(self.all_nodes)} ä¸ªèŠ‚ç‚¹")
                    return True
                else:
                    self.logger.warning("âš ï¸ é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰ä»£ç†èŠ‚ç‚¹ï¼Œåˆ‡æ¢åˆ°è‡ªåŠ¨è·å–æ¨¡å¼")

            # æ¨¡å¼2: é€šè¿‡custom_sourcesè‡ªåŠ¨è·å–èŠ‚ç‚¹
            self.logger.info("ğŸ“¥ æ¨¡å¼2: é€šè¿‡custom_sourcesè‡ªåŠ¨è·å–èŠ‚ç‚¹")
            self.logger.info(f"   èŠ‚ç‚¹æº: {list(self.custom_sources.keys())}")

            node_fetcher = NodeFetcher(custom_sources=self.custom_sources)
            self.all_nodes = await node_fetcher.fetch_nodes('all')

            if not self.all_nodes:
                self.logger.error("âŒ æœªè·å–åˆ°ä»»ä½•èŠ‚ç‚¹")
                return False

            self.logger.info(f"âœ… è‡ªåŠ¨è·å– {len(self.all_nodes)} ä¸ªèŠ‚ç‚¹")

            # æ˜¾ç¤ºèŠ‚ç‚¹ç±»å‹ç»Ÿè®¡
            node_types = {}
            for node in self.all_nodes:
                node_type = node.get('type', 'unknown')
                node_types[node_type] = node_types.get(node_type, 0) + 1

            self.logger.info(f"   èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ: {node_types}")
            return True

        except Exception as e:
            self.logger.error(f"âŒ æ­¥éª¤1å¤±è´¥: {e}")
            return False

    async def step2_create_initial_config(self) -> bool:
        """æ­¥éª¤2: åˆ›å»ºåˆå§‹é…ç½®å¹¶å†™å…¥rule"""
        self.logger.info("âš™ï¸ æ­¥éª¤2: åˆ›å»ºåˆå§‹é…ç½®")

        try:
            # ç”ŸæˆåŒ…å«æ‰€æœ‰èŠ‚ç‚¹çš„é…ç½®
            config = self._generate_smart_config(self.all_nodes)

            # ä¿å­˜é…ç½®
            with open(self.config_path, 'w', encoding='utf-8') as f:
                yaml.dump(config, f, default_flow_style=False, allow_unicode=True, indent=2)

            self.logger.info(f"âœ… åˆå§‹é…ç½®å·²ä¿å­˜: {self.config_path}")
            self.logger.info(f"   åŒ…å« {len(self.all_nodes)} ä¸ªä»£ç†èŠ‚ç‚¹")
            self.logger.info(f"   ä»£ç†è§„åˆ™: {len(self.proxy_urls)} ä¸ªåŸŸå")
            self.logger.info(f"   ç›´è¿è§„åˆ™: {len(self.direct_urls)} ä¸ªåŸŸå")

            return True

        except Exception as e:
            self.logger.error(f"âŒ æ­¥éª¤2å¤±è´¥: {e}")
            return False

    def _generate_smart_config(self, nodes: List[Dict]) -> Dict:
        """ç”Ÿæˆæ™ºèƒ½è·¯ç”±é…ç½®"""
        # ç¡®ä¿èŠ‚ç‚¹åç§°å”¯ä¸€
        unique_nodes = []
        used_names = set()

        for i, node in enumerate(nodes):
            original_name = node.get('name', f'node_{i}')
            name = original_name
            counter = 1

            while name in used_names:
                name = f"{original_name}_{counter}"
                counter += 1

            node['name'] = name
            used_names.add(name)
            unique_nodes.append(node)

        config = {
            'mixed-port': self.proxy_port,
            'allow-lan': False,
            'mode': 'rule',
            'log-level': 'info',
            'external-controller': f'127.0.0.1:{self.api_port}',
            'ipv6': True,
            'dns': {
                'enable': True,
                'nameserver': [
                    '8.8.8.8',
                    '1.1.1.1',
                    '114.114.114.114'
                ],
                'fallback': [
                    '8.8.4.4',
                    '1.0.0.1'
                ],
                'enhanced-mode': 'fake-ip',
                'fake-ip-range': '198.18.0.1/16',
                'fake-ip-filter': [
                    '*.lan',
                    'localhost.ptlogin2.qq.com',
                    '+.stun.*.*',
                    '+.stun.*.*.*',
                    '+.stun.*.*.*.*'
                ]
            },
            'proxies': unique_nodes,
            'proxy-groups': [
                {
                    'name': 'PROXY',
                    'type': 'select',  # ä½¿ç”¨selectç±»å‹ï¼Œæ›´ç¨³å®š
                    'proxies': ['DIRECT'] + [node['name'] for node in unique_nodes]
                },
                {
                    'name': 'AUTO',
                    'type': 'url-test',
                    'proxies': [node['name'] for node in unique_nodes] if unique_nodes else ['DIRECT'],
                    'url': 'http://www.gstatic.com/generate_204',
                    'interval': 300,
                    'tolerance': 50
                }
            ],
            'rules': self._generate_smart_rules()
        }

        return config

    def _generate_final_config_with_rules(self, nodes: List[Dict]) -> Dict:
        """ç”Ÿæˆæœ€ç»ˆé…ç½®ï¼ˆå¥åº·èŠ‚ç‚¹+æ–°é—»ç½‘ç»œè§„åˆ™ï¼‰"""
        # ç¡®ä¿èŠ‚ç‚¹åç§°å”¯ä¸€
        unique_nodes = []
        used_names = set()

        for i, node in enumerate(nodes):
            original_name = node.get('name', f'node_{i}')
            name = original_name
            counter = 1

            while name in used_names:
                name = f"{original_name}_{counter}"
                counter += 1

            node['name'] = name
            used_names.add(name)
            unique_nodes.append(node)

        config = {
            'mixed-port': self.proxy_port,
            'allow-lan': False,
            'mode': 'rule',
            'log-level': 'info',
            'external-controller': f'127.0.0.1:{self.api_port}',
            'ipv6': True,
            'dns': {
                'enable': True,
                'nameserver': [
                    '8.8.8.8',
                    '1.1.1.1',
                    '114.114.114.114'
                ],
                'fallback': [
                    '8.8.4.4',
                    '1.0.0.1'
                ],
                'enhanced-mode': 'fake-ip',
                'fake-ip-range': '198.18.0.1/16'
            },
            'proxies': unique_nodes,
            'proxy-groups': [
                {
                    'name': 'PROXY',
                    'type': 'select',
                    'proxies': [node['name'] for node in unique_nodes] + ['DIRECT']
                },
                {
                    'name': 'AUTO',
                    'type': 'url-test',
                    'proxies': [node['name'] for node in unique_nodes] if unique_nodes else ['DIRECT'],
                    'url': 'http://www.gstatic.com/generate_204',
                    'interval': 300,
                    'tolerance': 50
                }
            ],
            'rules': self._generate_news_crawling_rules()
        }

        return config

    def _generate_smart_rules(self) -> List[str]:
        """ç”Ÿæˆæ™ºèƒ½è·¯ç”±è§„åˆ™ - ä¿®å¤ç‰ˆæœ¬"""
        rules = []

        # 1. æœ¬åœ°ç½‘ç»œç›´è¿ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        rules.extend([
            'IP-CIDR,127.0.0.0/8,DIRECT',
            'IP-CIDR,172.16.0.0/12,DIRECT',
            'IP-CIDR,192.168.0.0/16,DIRECT',
            'IP-CIDR,10.0.0.0/8,DIRECT',
            'IP-CIDR,224.0.0.0/4,DIRECT',
            'DOMAIN-SUFFIX,local,DIRECT'
        ])

        # 2. å¥åº·æ£€æŸ¥å’Œæµ‹è¯•URLèµ°ä»£ç†ï¼ˆç¡®ä¿å¥åº·æ£€æŸ¥æ­£å¸¸å·¥ä½œï¼‰
        proxy_domains = [
            'httpbin.org',
            'www.gstatic.com',
            'gstatic.com',
            'detectportal.firefox.com',
            'www.msftconnecttest.com',
            'msftconnecttest.com',
            'google.com',
            'www.google.com'
        ]

        for domain in proxy_domains:
            rules.append(f'DOMAIN-SUFFIX,{domain},PROXY')

        # 3. æ–°é—»çˆ¬è™«ç›®æ ‡ç½‘ç«™èµ°ä»£ç†
        news_domains = [
            'panewslab.com',
            'www.panewslab.com'
        ]

        for domain in news_domains:
            rules.append(f'DOMAIN-SUFFIX,{domain},PROXY')

        # 4. å›½å†…ç½‘ç«™ç›´è¿
        direct_domains = [
            'baidu.com',
            'qq.com',
            'taobao.com',
            'jd.com',
            '163.com',
            'sina.com.cn',
            'weibo.com',
            'zhihu.com',
            'bilibili.com'
        ]

        for domain in direct_domains:
            rules.append(f'DOMAIN-SUFFIX,{domain},DIRECT')

        # 5. ä¸­å›½IPç›´è¿
        rules.append('GEOIP,CN,DIRECT')

        # 6. é»˜è®¤è§„åˆ™ï¼ˆå…¶ä»–æ‰€æœ‰æµé‡ç›´è¿ï¼‰
        rules.append('MATCH,DIRECT')

        return rules

    def _generate_news_crawling_rules(self) -> List[str]:
        """ç”Ÿæˆæ–°é—»çˆ¬å–ä¸“ç”¨è§„åˆ™"""
        rules = []

        # 1. æœ¬åœ°ç½‘ç»œç›´è¿ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        rules.extend([
            'IP-CIDR,127.0.0.0/8,DIRECT',
            'IP-CIDR,172.16.0.0/12,DIRECT',
            'IP-CIDR,192.168.0.0/16,DIRECT',
            'IP-CIDR,10.0.0.0/8,DIRECT',
            'DOMAIN-SUFFIX,local,DIRECT'
        ])

        # 2. æ–°é—»ç½‘ç«™å¿…é¡»èµ°ä»£ç†
        rules.extend([
            'DOMAIN-SUFFIX,panewslab.com,PROXY',
            'DOMAIN-SUFFIX,www.panewslab.com,PROXY'
        ])

        # 3. æµ‹è¯•å’Œå¥åº·æ£€æŸ¥URLèµ°ä»£ç†
        rules.extend([
            'DOMAIN-SUFFIX,httpbin.org,PROXY',
            'DOMAIN-SUFFIX,www.gstatic.com,PROXY',
            'DOMAIN-SUFFIX,gstatic.com,PROXY'
        ])

        # 4. å›½å†…ç½‘ç«™ç›´è¿
        rules.extend([
            'DOMAIN-SUFFIX,baidu.com,DIRECT',
            'DOMAIN-SUFFIX,qq.com,DIRECT',
            'DOMAIN-SUFFIX,taobao.com,DIRECT',
            'DOMAIN-SUFFIX,jd.com,DIRECT'
        ])

        # 5. ä¸­å›½IPç›´è¿
        rules.append('GEOIP,CN,DIRECT')

        # 6. é»˜è®¤è§„åˆ™ï¼ˆå…¶ä»–æµé‡ç›´è¿ï¼‰
        rules.append('MATCH,DIRECT')

        return rules

    def _generate_news_crawling_rules(self) -> List[str]:
        """ç”Ÿæˆæ–°é—»çˆ¬å–ä¸“ç”¨è§„åˆ™"""
        rules = []

        # 1. æœ¬åœ°ç½‘ç»œç›´è¿ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        rules.extend([
            'IP-CIDR,127.0.0.0/8,DIRECT',
            'IP-CIDR,172.16.0.0/12,DIRECT',
            'IP-CIDR,192.168.0.0/16,DIRECT',
            'IP-CIDR,10.0.0.0/8,DIRECT',
            'DOMAIN-SUFFIX,local,DIRECT'
        ])

        # 2. æ–°é—»ç½‘ç«™å¿…é¡»èµ°ä»£ç†
        rules.extend([
            'DOMAIN-SUFFIX,panewslab.com,PROXY',
            'DOMAIN-SUFFIX,www.panewslab.com,PROXY'
        ])

        # 3. æµ‹è¯•å’Œå¥åº·æ£€æŸ¥URLèµ°ä»£ç†
        rules.extend([
            'DOMAIN-SUFFIX,httpbin.org,PROXY',
            'DOMAIN-SUFFIX,www.gstatic.com,PROXY',
            'DOMAIN-SUFFIX,gstatic.com,PROXY'
        ])

        # 4. å›½å†…ç½‘ç«™ç›´è¿
        rules.extend([
            'DOMAIN-SUFFIX,baidu.com,DIRECT',
            'DOMAIN-SUFFIX,qq.com,DIRECT',
            'DOMAIN-SUFFIX,taobao.com,DIRECT',
            'DOMAIN-SUFFIX,jd.com,DIRECT'
        ])

        # 5. ä¸­å›½IPç›´è¿
        rules.append('GEOIP,CN,DIRECT')

        # 6. é»˜è®¤è§„åˆ™ï¼ˆå…¶ä»–æµé‡ç›´è¿ï¼‰
        rules.append('MATCH,DIRECT')

        return rules

    async def step3_test_nodes_health(self) -> bool:
        """æ­¥éª¤3: æµ‹è¯•èŠ‚ç‚¹å¥åº·çŠ¶å†µï¼ˆä¸å¯åŠ¨Clashï¼‰"""
        self.logger.info("ğŸ¥ æ­¥éª¤3: æµ‹è¯•èŠ‚ç‚¹å¥åº·çŠ¶å†µ")

        try:
            # 3.1 ç›´æ¥æµ‹è¯•èŠ‚ç‚¹è¿é€šæ€§ï¼ˆä¸ä¾èµ–Clashï¼‰
            self.logger.info("ğŸ” ç›´æ¥æµ‹è¯•èŠ‚ç‚¹è¿é€šæ€§...")
            health_results = await self._test_nodes_directly()

            if not health_results:
                self.logger.warning("âš ï¸ å¥åº·æ£€æŸ¥æœªè¿”å›ç»“æœ")
                return False

            # 3.2 ç­›é€‰å¥åº·èŠ‚ç‚¹
            self.healthy_nodes = self._filter_healthy_nodes(health_results)

            healthy_count = len(self.healthy_nodes)
            total_count = len(self.all_nodes)

            self.logger.info(f"âœ… å¥åº·æ£€æŸ¥å®Œæˆ: {healthy_count}/{total_count} ä¸ªèŠ‚ç‚¹å¥åº·")

            # 3.3 æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å¥åº·èŠ‚ç‚¹
            if healthy_count < self.min_healthy_nodes:
                self.logger.warning(f"âš ï¸ å¥åº·èŠ‚ç‚¹æ•°é‡ ({healthy_count}) å°‘äºæœ€å°è¦æ±‚ ({self.min_healthy_nodes})")
                self.logger.info("   å°†é™ä½å¥åº·æ ‡å‡†é‡æ–°ç­›é€‰...")

                # é™ä½å¥åº·æ ‡å‡†
                self.healthy_nodes = self._filter_healthy_nodes(health_results, threshold=0.1)
                healthy_count = len(self.healthy_nodes)
                self.logger.info(f"   é™ä½æ ‡å‡†å: {healthy_count} ä¸ªèŠ‚ç‚¹å¯ç”¨")

            if healthy_count == 0:
                self.logger.error("âŒ æ²¡æœ‰å¯ç”¨çš„å¥åº·èŠ‚ç‚¹")
                return False

            # 3.4 æ˜¾ç¤ºå¥åº·èŠ‚ç‚¹ä¿¡æ¯
            self.logger.info("âœ… å¥åº·èŠ‚ç‚¹åˆ—è¡¨:")
            for i, node in enumerate(self.healthy_nodes[:5], 1):
                node_name = node.get('name', f'node_{i}')
                node_type = node.get('type', 'unknown')
                server = node.get('server', 'unknown')
                self.logger.info(f"   {i}. {node_name} ({node_type}) - {server}")

            return True

        except Exception as e:
            self.logger.error(f"âŒ æ­¥éª¤3å¤±è´¥: {e}")
            return False

    async def _start_clash(self) -> bool:
        """å¯åŠ¨Clashè¿›ç¨‹"""
        try:
            if not self.binary_path.exists():
                self.logger.error(f"âŒ äºŒè¿›åˆ¶æ–‡ä»¶ä¸å­˜åœ¨: {self.binary_path}")
                return False

            cmd = [str(self.binary_path), '-f', str(self.config_path)]
            self.logger.info(f"å¯åŠ¨å‘½ä»¤: {' '.join(cmd)}")

            self.clash_process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                creationflags=subprocess.CREATE_NO_WINDOW if platform.system() == 'Windows' else 0
            )

            # æ£€æŸ¥å¯åŠ¨çŠ¶æ€
            await asyncio.sleep(3)

            if self.clash_process.poll() is None:
                self.logger.info("âœ… Clashå¯åŠ¨æˆåŠŸ")
                return True
            else:
                stdout, stderr = self.clash_process.communicate()
                self.logger.error("âŒ Clashå¯åŠ¨å¤±è´¥")
                if stdout:
                    self.logger.error(f"æ ‡å‡†è¾“å‡º: {stdout.decode('utf-8', errors='ignore')}")
                if stderr:
                    self.logger.error(f"é”™è¯¯è¾“å‡º: {stderr.decode('utf-8', errors='ignore')}")
                return False

        except Exception as e:
            self.logger.error(f"âŒ å¯åŠ¨Clashå¼‚å¸¸: {e}")
            return False

    async def _perform_comprehensive_health_check(self) -> Dict[str, float]:
        """æ‰§è¡Œå…¨é¢çš„å¥åº·æ£€æŸ¥"""
        self.logger.info("ğŸ” æ‰§è¡Œå…¨é¢å¥åº·æ£€æŸ¥...")

        health_results = {}

        try:
            # è·å–ä»£ç†åˆ—è¡¨
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{self.clash_api_base}/proxies") as response:
                    if response.status != 200:
                        self.logger.error("âŒ æ— æ³•è·å–ä»£ç†åˆ—è¡¨")
                        return {}

                    data = await response.json()
                    proxies = data.get('proxies', {})

                    if 'PROXY' not in proxies:
                        self.logger.error("âŒ æ‰¾ä¸åˆ°PROXYç»„")
                        return {}

                    proxy_names = proxies['PROXY'].get('all', [])
                    actual_proxies = [p for p in proxy_names if p not in ['DIRECT']]

                    self.logger.info(f"ğŸ“‹ æ‰¾åˆ° {len(actual_proxies)} ä¸ªä»£ç†è¿›è¡Œå¥åº·æ£€æŸ¥")

                    # åˆ†æ‰¹æµ‹è¯•ä»£ç†ï¼ˆé¿å…è¿‡å¤šå¹¶å‘ï¼‰
                    batch_size = 5
                    for i in range(0, len(actual_proxies), batch_size):
                        batch = actual_proxies[i:i+batch_size]
                        self.logger.info(f"ğŸ” æµ‹è¯•æ‰¹æ¬¡ {i//batch_size + 1}: {len(batch)} ä¸ªä»£ç†")

                        # å¹¶å‘æµ‹è¯•å½“å‰æ‰¹æ¬¡
                        tasks = [self._test_single_proxy_comprehensive(proxy_name) for proxy_name in batch]
                        batch_results = await asyncio.gather(*tasks, return_exceptions=True)

                        # å¤„ç†ç»“æœ
                        for proxy_name, result in zip(batch, batch_results):
                            if isinstance(result, Exception):
                                health_results[proxy_name] = 0.0
                                self.logger.debug(f"âŒ {proxy_name}: æµ‹è¯•å¼‚å¸¸ ({result})")
                            else:
                                health_results[proxy_name] = result
                                if result > 0:
                                    self.logger.info(f"âœ… {proxy_name}: {result:.2f}")
                                else:
                                    self.logger.debug(f"âŒ {proxy_name}: ä¸å¯ç”¨")

                        # æ‰¹æ¬¡é—´å»¶è¿Ÿ
                        if i + batch_size < len(actual_proxies):
                            await asyncio.sleep(2)

        except Exception as e:
            self.logger.error(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {e}")

        return health_results

    async def _test_single_proxy_comprehensive(self, proxy_name: str) -> float:
        """å…¨é¢æµ‹è¯•å•ä¸ªä»£ç†"""
        try:
            # åˆ‡æ¢åˆ°æŒ‡å®šä»£ç†
            async with aiohttp.ClientSession() as session:
                switch_data = {"name": proxy_name}
                async with session.put(f"{self.clash_api_base}/proxies/PROXY", json=switch_data) as response:
                    if response.status != 204:
                        return 0.0

                # ç­‰å¾…åˆ‡æ¢ç”Ÿæ•ˆ
                await asyncio.sleep(1)

                # å¤šé‡æµ‹è¯•
                test_results = []

                # æµ‹è¯•1: åŸºç¡€è¿é€šæ€§
                basic_score = await self._test_basic_connectivity()
                test_results.append(basic_score)

                # æµ‹è¯•2: HTTPSè¿æ¥
                https_score = await self._test_https_connectivity()
                test_results.append(https_score)

                # æµ‹è¯•3: ç›®æ ‡ç½‘ç«™è®¿é—®
                target_score = await self._test_target_website_access()
                test_results.append(target_score)

                # è®¡ç®—ç»¼åˆåˆ†æ•°
                valid_results = [score for score in test_results if score >= 0]
                if valid_results:
                    final_score = sum(valid_results) / len(valid_results)
                    return final_score
                else:
                    return 0.0

        except Exception as e:
            self.logger.debug(f"ä»£ç† {proxy_name} æµ‹è¯•å¤±è´¥: {e}")
            return 0.0

    async def _test_basic_connectivity(self) -> float:
        """æµ‹è¯•åŸºç¡€è¿é€šæ€§ - ä½¿ç”¨requestsåº“æ›´å¯é """
        test_urls = [
            'http://httpbin.org/ip',
            'http://www.gstatic.com/generate_204'
        ]

        success_count = 0
        for url in test_urls:
            try:
                # ä½¿ç”¨requestsåº“è¿›è¡Œæµ‹è¯•ï¼Œæ›´ç¨³å®š
                import requests
                proxies = {
                    'http': f'http://127.0.0.1:{self.proxy_port}',
                    'https': f'http://127.0.0.1:{self.proxy_port}'
                }

                response = requests.get(url, proxies=proxies, timeout=10)
                if response.status_code in [200, 204]:
                    success_count += 1
                    self.logger.debug(f"âœ… åŸºç¡€è¿é€šæ€§æµ‹è¯•æˆåŠŸ: {url}")
                else:
                    self.logger.debug(f"âŒ åŸºç¡€è¿é€šæ€§æµ‹è¯•å¤±è´¥: {url} - HTTP {response.status_code}")
            except Exception as e:
                self.logger.debug(f"âŒ åŸºç¡€è¿é€šæ€§æµ‹è¯•å¼‚å¸¸: {url} - {e}")
                continue

        return success_count / len(test_urls)

    async def _test_https_connectivity(self) -> float:
        """æµ‹è¯•HTTPSè¿é€šæ€§ - ä½¿ç”¨requestsåº“æ›´å¯é """
        test_urls = [
            'https://www.google.com/generate_204',
            'https://httpbin.org/ip'
        ]

        success_count = 0
        for url in test_urls:
            try:
                # ä½¿ç”¨requestsåº“è¿›è¡ŒHTTPSæµ‹è¯•
                import requests
                proxies = {
                    'http': f'http://127.0.0.1:{self.proxy_port}',
                    'https': f'http://127.0.0.1:{self.proxy_port}'
                }

                response = requests.get(url, proxies=proxies, timeout=15, verify=False)
                if response.status_code in [200, 204]:
                    success_count += 1
                    self.logger.debug(f"âœ… HTTPSè¿é€šæ€§æµ‹è¯•æˆåŠŸ: {url}")
                else:
                    self.logger.debug(f"âŒ HTTPSè¿é€šæ€§æµ‹è¯•å¤±è´¥: {url} - HTTP {response.status_code}")
            except Exception as e:
                self.logger.debug(f"âŒ HTTPSè¿é€šæ€§æµ‹è¯•å¼‚å¸¸: {url} - {e}")
                continue

        return success_count / len(test_urls)

    async def _test_target_website_access(self) -> float:
        """æµ‹è¯•ç›®æ ‡ç½‘ç«™è®¿é—® - ä½¿ç”¨requestsåº“æ›´å¯é """
        try:
            # ä½¿ç”¨requestsåº“æµ‹è¯•ç›®æ ‡ç½‘ç«™
            import requests
            proxies = {
                'http': f'http://127.0.0.1:{self.proxy_port}',
                'https': f'http://127.0.0.1:{self.proxy_port}'
            }

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }

            response = requests.get(
                "https://www.panewslab.com/webapi/flashnews?LId=1&Rn=1&tw=0",
                proxies=proxies,
                headers=headers,
                timeout=20,
                verify=False
            )

            if response.status_code == 200:
                self.logger.debug("âœ… ç›®æ ‡ç½‘ç«™è®¿é—®æµ‹è¯•æˆåŠŸ")
                return 1.0
            elif response.status_code in [301, 302, 403]:
                self.logger.debug(f"âš ï¸ ç›®æ ‡ç½‘ç«™è®¿é—®éƒ¨åˆ†æˆåŠŸ: HTTP {response.status_code}")
                return 0.5  # éƒ¨åˆ†å¯ç”¨
            else:
                self.logger.debug(f"âŒ ç›®æ ‡ç½‘ç«™è®¿é—®å¤±è´¥: HTTP {response.status_code}")
                return 0.0
        except Exception as e:
            self.logger.debug(f"âŒ ç›®æ ‡ç½‘ç«™è®¿é—®å¼‚å¸¸: {e}")
            return 0.0

    async def _test_nodes_directly(self) -> Dict[str, float]:
        """ç›´æ¥æµ‹è¯•èŠ‚ç‚¹å¥åº·çŠ¶å†µï¼ˆä¸ä¾èµ–Clashï¼‰"""
        self.logger.info("ğŸ” ç›´æ¥æµ‹è¯•èŠ‚ç‚¹è¿é€šæ€§...")

        health_results = {}

        try:
            # åˆ†æ‰¹æµ‹è¯•èŠ‚ç‚¹ï¼ˆé¿å…è¿‡å¤šå¹¶å‘ï¼‰
            batch_size = 3
            for i in range(0, len(self.all_nodes), batch_size):
                batch = self.all_nodes[i:i+batch_size]
                self.logger.info(f"ğŸ” æµ‹è¯•æ‰¹æ¬¡ {i//batch_size + 1}: {len(batch)} ä¸ªèŠ‚ç‚¹")

                # å¹¶å‘æµ‹è¯•å½“å‰æ‰¹æ¬¡
                tasks = [self._test_single_node_directly(node) for node in batch]
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)

                # å¤„ç†ç»“æœ
                for node, result in zip(batch, batch_results):
                    node_name = node.get('name', f'node_{i}')
                    if isinstance(result, Exception):
                        health_results[node_name] = 0.0
                        self.logger.debug(f"âŒ {node_name}: æµ‹è¯•å¼‚å¸¸ ({result})")
                    else:
                        health_results[node_name] = result
                        if result > 0:
                            self.logger.info(f"âœ… {node_name}: {result:.2f}")
                        else:
                            self.logger.debug(f"âŒ {node_name}: ä¸å¯ç”¨")

                # æ‰¹æ¬¡é—´å»¶è¿Ÿ
                if i + batch_size < len(self.all_nodes):
                    await asyncio.sleep(1)

        except Exception as e:
            self.logger.error(f"âŒ ç›´æ¥èŠ‚ç‚¹æµ‹è¯•å¤±è´¥: {e}")

        return health_results

    async def _test_single_node_directly(self, node: Dict) -> float:
        """ç›´æ¥æµ‹è¯•å•ä¸ªèŠ‚ç‚¹ï¼ˆä¸ä¾èµ–Clashï¼‰"""
        try:
            node_name = node.get('name', 'unknown')
            node_type = node.get('type', 'unknown')
            server = node.get('server', '')
            port = node.get('port', 0)

            if not server or not port:
                return 0.0

            # ç®€å•çš„TCPè¿æ¥æµ‹è¯•
            import socket
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(5)

            try:
                result = sock.connect_ex((server, port))
                if result == 0:
                    self.logger.debug(f"âœ… {node_name}: TCPè¿æ¥æˆåŠŸ")
                    return 0.8  # åŸºç¡€è¿é€šæ€§åˆ†æ•°
                else:
                    self.logger.debug(f"âŒ {node_name}: TCPè¿æ¥å¤±è´¥")
                    return 0.0
            finally:
                sock.close()

        except Exception as e:
            self.logger.debug(f"âŒ {node_name}: æµ‹è¯•å¼‚å¸¸ - {e}")
            return 0.0

    def _filter_healthy_nodes(self, health_results: Dict[str, float], threshold: float = 0.3) -> List[Dict]:
        """ç­›é€‰å¥åº·èŠ‚ç‚¹"""
        healthy_nodes = []

        for node in self.all_nodes:
            node_name = node.get('name', '')
            score = health_results.get(node_name, 0.0)

            if score >= threshold:
                healthy_nodes.append(node)

        return healthy_nodes

    async def step4_create_final_config_and_start_clash(self) -> bool:
        """æ­¥éª¤4: åˆ›å»ºæœ€ç»ˆé…ç½®ï¼ˆå¥åº·èŠ‚ç‚¹+è§„åˆ™ï¼‰å¹¶å¯åŠ¨Clash"""
        self.logger.info("âš™ï¸ æ­¥éª¤4: åˆ›å»ºæœ€ç»ˆé…ç½®å¹¶å¯åŠ¨Clash")

        try:
            # 4.1 ç”Ÿæˆæœ€ç»ˆé…ç½®ï¼ˆåªåŒ…å«å¥åº·èŠ‚ç‚¹+æ–°é—»ç½‘ç»œè§„åˆ™ï¼‰
            if self.healthy_nodes:
                final_config = self._generate_final_config_with_rules(self.healthy_nodes)
                self.logger.info(f"âœ… ä½¿ç”¨ {len(self.healthy_nodes)} ä¸ªå¥åº·èŠ‚ç‚¹ç”Ÿæˆæœ€ç»ˆé…ç½®")
            else:
                self.logger.warning("âš ï¸ æ²¡æœ‰å¥åº·èŠ‚ç‚¹ï¼Œä½¿ç”¨æ‰€æœ‰èŠ‚ç‚¹")
                final_config = self._generate_final_config_with_rules(self.all_nodes)

            # 4.2 ä¿å­˜æœ€ç»ˆé…ç½®
            with open(self.config_path, 'w', encoding='utf-8') as f:
                yaml.dump(final_config, f, default_flow_style=False, allow_unicode=True, indent=2)

            self.logger.info(f"âœ… æœ€ç»ˆé…ç½®å·²ä¿å­˜: {self.config_path}")
            self.logger.info(f"   åŒ…å« {len(final_config['proxies'])} ä¸ªå¥åº·èŠ‚ç‚¹")
            self.logger.info(f"   æ–°é—»ç½‘ç»œè§„åˆ™: panewslab.com -> PROXY")
            self.logger.info(f"   å›½å†…ç½‘ç»œè§„åˆ™: baidu.comç­‰ -> DIRECT")

            # 4.3 å¯åŠ¨Clash
            if not await self._start_clash():
                return False

            # 4.4 ç­‰å¾…å¯åŠ¨å®Œæˆ
            await asyncio.sleep(5)

            # 4.5 éªŒè¯Clash APIå¯ç”¨
            if not await self._verify_clash_api():
                return False

            self.logger.info("âœ… Clashå¯åŠ¨å®Œæˆï¼Œå‡†å¤‡å¼€å§‹æ–°é—»çˆ¬å–")
            return True

        except Exception as e:
            self.logger.error(f"âŒ æ­¥éª¤4å¤±è´¥: {e}")
            return False

    async def step5_crawler_test_with_switching(self) -> bool:
        """æ­¥éª¤5: çˆ¬è™«æµ‹è¯•å’Œä»£ç†åˆ‡æ¢"""
        self.logger.info("ğŸ•·ï¸ æ­¥éª¤5: çˆ¬è™«æµ‹è¯•å’Œä»£ç†åˆ‡æ¢")

        try:
            # 5.1 ç¬¬ä¸€æ¬¡çˆ¬å–
            self.logger.info("ğŸ“° ç¬¬ä¸€æ¬¡æ–°é—»çˆ¬å–...")
            ip1, news1 = await self._fetch_news_with_ip_check()

            if not news1:
                self.logger.warning("âš ï¸ ç¬¬ä¸€æ¬¡çˆ¬å–å¤±è´¥ï¼Œå°è¯•ç›´è¿æ¨¡å¼")
                # å°è¯•ç›´è¿æ¨¡å¼
                direct_news = await self._test_direct_news_access()
                if direct_news:
                    self.logger.info("âœ… ç›´è¿æ¨¡å¼å¯ä»¥è®¿é—®æ–°é—»API")
                else:
                    self.logger.error("âŒ è¿ç›´è¿æ¨¡å¼éƒ½æ— æ³•è®¿é—®æ–°é—»API")
                return False

            self.logger.info(f"âœ… ç¬¬ä¸€æ¬¡çˆ¬å–æˆåŠŸï¼ŒIP: {ip1}, æ–°é—»æ•°: {len(news1)}")

            # 5.2 å¼ºåˆ¶åˆ‡æ¢ä»£ç†
            self.logger.info("ğŸ”„ å¼ºåˆ¶åˆ‡æ¢ä»£ç†...")
            switch_success = await self._force_switch_proxy()

            if not switch_success:
                self.logger.warning("âš ï¸ ä»£ç†åˆ‡æ¢å¤±è´¥ï¼Œå¯èƒ½åªæœ‰ä¸€ä¸ªå¯ç”¨ä»£ç†")

            # 5.3 ç­‰å¾…åˆ‡æ¢ç”Ÿæ•ˆ
            await asyncio.sleep(3)

            # 5.4 ç¬¬äºŒæ¬¡çˆ¬å–
            self.logger.info("ğŸ“° ç¬¬äºŒæ¬¡æ–°é—»çˆ¬å–...")
            ip2, news2 = await self._fetch_news_with_ip_check()

            if not news2:
                self.logger.warning("âš ï¸ ç¬¬äºŒæ¬¡çˆ¬å–å¤±è´¥")
                return False

            self.logger.info(f"âœ… ç¬¬äºŒæ¬¡çˆ¬å–æˆåŠŸï¼ŒIP: {ip2}, æ–°é—»æ•°: {len(news2)}")

            # 5.5 éªŒè¯IPåˆ‡æ¢
            if ip1 != ip2:
                self.logger.info("ğŸ‰ ä»£ç†åˆ‡æ¢æˆåŠŸï¼IPå·²æ”¹å˜")
            else:
                self.logger.warning("âš ï¸ IPæœªæ”¹å˜ï¼Œå¯èƒ½åˆ‡æ¢å¤±è´¥æˆ–ä»£ç†ç›¸åŒ")

            # 5.6 æ˜¾ç¤ºæ–°é—»å†…å®¹ç¤ºä¾‹
            if news1 and len(news1) > 0:
                try:
                    first_news = news1[0]
                    # å°è¯•ä¸åŒçš„å­—æ®µå
                    title = first_news.get('title') or first_news.get('Title') or first_news.get('content', 'No title')
                    if isinstance(title, str) and len(title) > 0:
                        self.logger.info(f"ğŸ“° æ–°é—»ç¤ºä¾‹: {title[:50]}...")
                    else:
                        self.logger.info(f"ğŸ“° æ–°é—»æ•°æ®ç»“æ„: {list(first_news.keys()) if isinstance(first_news, dict) else type(first_news)}")
                except Exception as e:
                    self.logger.debug(f"æ˜¾ç¤ºæ–°é—»ç¤ºä¾‹æ—¶å‡ºé”™: {e}")

            return True

        except Exception as e:
            self.logger.error(f"âŒ æ­¥éª¤5å¤±è´¥: {e}")
            return False

    async def _fetch_news_with_ip_check(self) -> tuple:
        """è·å–æ–°é—»å¹¶æ£€æŸ¥IP"""
        try:
            # æ£€æŸ¥å½“å‰IP
            current_ip = await self._get_current_ip()

            # è·å–æ–°é—»
            news_data = await self._fetch_panews_data()

            return current_ip, news_data

        except Exception as e:
            self.logger.error(f"æ–°é—»çˆ¬å–å¤±è´¥: {e}")
            return None, None

    async def _get_current_ip(self) -> str:
        """è·å–å½“å‰IPåœ°å€"""
        try:
            proxy_url = f"http://127.0.0.1:{self.proxy_port}"
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
                async with session.get("http://httpbin.org/ip", proxy=proxy_url) as response:
                    if response.status == 200:
                        data = await response.json()
                        return data.get('origin', 'unknown')
        except Exception as e:
            self.logger.debug(f"è·å–IPå¤±è´¥: {e}")

        return 'unknown'

    async def _fetch_panews_data(self) -> List[Dict]:
        """è·å–PanewsLabæ–°é—»æ•°æ®"""
        try:
            proxy_url = f"http://127.0.0.1:{self.proxy_port}"
            url = "https://www.panewslab.com/webapi/flashnews?LId=1&Rn=5&tw=0"

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }

            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=20)) as session:
                async with session.get(url, proxy=proxy_url, headers=headers) as response:
                    if response.status == 200:
                        data = await response.json()
                        return data.get('data', [])
                    else:
                        self.logger.warning(f"æ–°é—»APIè¿”å›çŠ¶æ€: {response.status}")
                        return []
        except Exception as e:
            self.logger.debug(f"è·å–æ–°é—»æ•°æ®å¤±è´¥: {e}")
            return []

    async def _test_direct_news_access(self) -> bool:
        """æµ‹è¯•ç›´è¿è®¿é—®æ–°é—»API"""
        try:
            url = "https://www.panewslab.com/webapi/flashnews?LId=1&Rn=3&tw=0"
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=15)) as session:
                async with session.get(url) as response:
                    if response.status == 200:
                        data = await response.json()
                        news_count = len(data.get('data', []))
                        self.logger.info(f"âœ… ç›´è¿è®¿é—®æˆåŠŸï¼Œè·å– {news_count} æ¡æ–°é—»")
                        return True
                    else:
                        self.logger.warning(f"ç›´è¿è®¿é—®å¤±è´¥: {response.status}")
                        return False
        except Exception as e:
            self.logger.error(f"ç›´è¿è®¿é—®å¼‚å¸¸: {e}")
            return False

    async def _force_switch_proxy(self) -> bool:
        """å¼ºåˆ¶åˆ‡æ¢ä»£ç†"""
        try:
            # è·å–å¯ç”¨ä»£ç†åˆ—è¡¨
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{self.clash_api_base}/proxies/PROXY") as response:
                    if response.status == 200:
                        data = await response.json()
                        current_proxy = data.get('now', '')
                        all_proxies = data.get('all', [])

                        # é€‰æ‹©ä¸åŒçš„ä»£ç†
                        available_proxies = [p for p in all_proxies if p != current_proxy and p != 'DIRECT']

                        if available_proxies:
                            new_proxy = available_proxies[0]

                            # åˆ‡æ¢ä»£ç†
                            switch_data = {"name": new_proxy}
                            async with session.put(f"{self.clash_api_base}/proxies/PROXY", json=switch_data) as switch_response:
                                if switch_response.status == 204:
                                    self.logger.info(f"âœ… ä»£ç†åˆ‡æ¢: {current_proxy} â†’ {new_proxy}")
                                    return True
                                else:
                                    self.logger.warning(f"âš ï¸ ä»£ç†åˆ‡æ¢å¤±è´¥: HTTP {switch_response.status}")
                        else:
                            self.logger.warning("âš ï¸ æ²¡æœ‰å…¶ä»–å¯ç”¨ä»£ç†")

        except Exception as e:
            self.logger.error(f"å¼ºåˆ¶åˆ‡æ¢ä»£ç†å¤±è´¥: {e}")

        return False

    async def step6_verify_direct_connection(self) -> bool:
        """æ­¥éª¤6: éªŒè¯å…¶ä»–ç½‘å€ä¸èµ°ä»£ç†"""
        self.logger.info("ğŸŒ æ­¥éª¤6: éªŒè¯ç›´è¿ç½‘å€")

        try:
            success_count = 0

            for domain in self.direct_urls:
                url = f"http://{domain}"

                try:
                    # ä¸ä½¿ç”¨ä»£ç†ç›´æ¥è®¿é—®
                    async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
                        async with session.get(url) as response:
                            if response.status in [200, 301, 302]:
                                self.logger.info(f"âœ… ç›´è¿æˆåŠŸ: {domain}")
                                success_count += 1
                            else:
                                self.logger.warning(f"âš ï¸ ç›´è¿çŠ¶æ€å¼‚å¸¸: {domain} - {response.status}")
                except Exception as e:
                    self.logger.debug(f"ç›´è¿æµ‹è¯•å¤±è´¥: {domain} - {e}")

            self.logger.info(f"âœ… ç›´è¿éªŒè¯å®Œæˆ: {success_count}/{len(self.direct_urls)} ä¸ªç½‘ç«™å¯è®¿é—®")
            return True

        except Exception as e:
            self.logger.error(f"âŒ æ­¥éª¤6å¤±è´¥: {e}")
            return False

    async def _stop_clash(self):
        """åœæ­¢Clashè¿›ç¨‹"""
        if self.clash_process:
            try:
                self.clash_process.terminate()
                self.clash_process.wait(timeout=5)
                self.logger.info("âœ… Clashå·²åœæ­¢")
            except subprocess.TimeoutExpired:
                self.clash_process.kill()
                self.logger.info("ğŸ”ª Clashå·²å¼ºåˆ¶åœæ­¢")
            finally:
                self.clash_process = None

    async def _verify_clash_api(self) -> bool:
        """éªŒè¯Clash APIæ˜¯å¦å¯ç”¨"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{self.clash_api_base}/proxies") as response:
                    if response.status == 200:
                        data = await response.json()
                        if 'proxies' in data and 'PROXY' in data['proxies']:
                            self.logger.info("âœ… Clash APIéªŒè¯æˆåŠŸ")
                            return True
                        else:
                            self.logger.error("âŒ Clash APIå“åº”æ ¼å¼å¼‚å¸¸")
                            return False
                    else:
                        self.logger.error(f"âŒ Clash APIä¸å¯ç”¨: HTTP {response.status}")
                        return False
        except Exception as e:
            self.logger.error(f"âŒ Clash APIéªŒè¯å¤±è´¥: {e}")
            return False

    async def run_complete_test(self) -> bool:
        """è¿è¡Œå®Œæ•´æµ‹è¯•æµç¨‹"""
        self.logger.info("ğŸ¯ å¼€å§‹æ”¹è¿›çš„å®Œæ•´æ–°é—»çˆ¬è™«æµ‹è¯•")
        self.logger.info("=" * 80)

        try:
            # æ­¥éª¤1: èŠ‚ç‚¹è·å–
            if not await self.step1_fetch_nodes():
                return False

            # æ­¥éª¤2: åˆ›å»ºåˆå§‹é…ç½®
            if not await self.step2_create_initial_config():
                return False

            # æ­¥éª¤3: æµ‹è¯•èŠ‚ç‚¹å¥åº·çŠ¶å†µï¼ˆä¸å¯åŠ¨Clashï¼‰
            if not await self.step3_test_nodes_health():
                return False

            # æ­¥éª¤4: åˆ›å»ºæœ€ç»ˆé…ç½®å¹¶å¯åŠ¨Clash
            if not await self.step4_create_final_config_and_start_clash():
                return False

            # æ­¥éª¤5: çˆ¬è™«æµ‹è¯•å’Œä»£ç†åˆ‡æ¢
            if not await self.step5_crawler_test_with_switching():
                return False

            # æ­¥éª¤6: éªŒè¯ç›´è¿
            if not await self.step6_verify_direct_connection():
                return False

            self.logger.info("ğŸ‰ å®Œæ•´æµ‹è¯•æµç¨‹æˆåŠŸï¼")
            return True

        except Exception as e:
            self.logger.error(f"âŒ å®Œæ•´æµ‹è¯•å¤±è´¥: {e}")
            return False
        finally:
            # æ¸…ç†èµ„æº
            await self._stop_clash()

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        await self._stop_clash()


async def main():
    """ä¸»å‡½æ•°"""
    # æµ‹è¯•æ¨¡å¼1: ä½¿ç”¨custom_sourcesè‡ªåŠ¨è·å–èŠ‚ç‚¹
    print("ğŸ”§ æµ‹è¯•æ¨¡å¼1: ä½¿ç”¨custom_sourcesè‡ªåŠ¨è·å–èŠ‚ç‚¹")

    custom_sources = {
        'clash': [
            'https://7izza.no-mad-world.club/link/iHil1Ll4I1XzfGDW?clash=3&extend=1',
        ],
        'v2ray': []
    }

    tester1 = ImprovedCompleteNewsTest(
        custom_sources=custom_sources,
        min_healthy_nodes=2  # é™ä½æœ€å°è¦æ±‚
    )

    try:
        success1 = await tester1.run_complete_test()

        if success1:
            print("\nâœ… æ¨¡å¼1æµ‹è¯•æˆåŠŸï¼")
            print("æ‰€æœ‰åŠŸèƒ½éƒ½æ­£å¸¸å·¥ä½œï¼š")
            print("  - è‡ªåŠ¨èŠ‚ç‚¹è·å–æ­£å¸¸")
            print("  - æ™ºèƒ½è·¯ç”±è§„åˆ™ç”Ÿæ•ˆ")
            print("  - ä»£ç†è‡ªåŠ¨åˆ‡æ¢æ­£å¸¸")
            print("  - æ–°é—»çˆ¬å–åŠŸèƒ½æ­£å¸¸")
            print("  - ç›´è¿ç½‘å€ä¸èµ°ä»£ç†")
        else:
            print("\nâŒ æ¨¡å¼1æµ‹è¯•å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—äº†è§£è¯¦æƒ…ã€‚")

    except KeyboardInterrupt:
        print("\nâ¹ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    finally:
        await tester1.cleanup()


if __name__ == "__main__":
    asyncio.run(main())